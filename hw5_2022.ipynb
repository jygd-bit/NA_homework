{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "535c17f1-f813-4c48-9bd8-dfc3856c9d07",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-06d3e5bf55c941ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Homework set 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3bf949-9252-4592-bde1-fab154d5018d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-736ff6bc3e0d0696",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected (in the menubar, select Kernel → Restart Kernel and Run All Cells...).\n",
    "\n",
    "Please **submit this Jupyter notebook through Canvas** no later than **Mon Dec. 5, 9:00**. **Submit the notebook file with your answers (as .ipynb file) and a pdf printout. The pdf version can be used by the teachers to provide feedback. A pdf version can be made using the save and export option in the Jupyter Lab file menu.**\n",
    "\n",
    "Homework is in **groups of two**, and you are expected to hand in original work. Work that is copied from another group will not be accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f057a-3ad9-4b58-848b-690aab7d7de2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b13bc5ed16bce8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 0\n",
    "Write down the names + student ID of the people in your group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdaace4-9cfe-41cb-b26b-fee6914deec4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fd464f55ba436b1c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Jade Dubbeld, 11692065\n",
    "\n",
    "Maickel Hartlief, 14015277"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd967a45-d51e-46ae-a464-402584619799",
   "metadata": {},
   "source": [
    "# The global keyword (helpful info for exercise 2)\n",
    "In exercise 2 you are asked, at some point, to count the number of times a certain function is evaluated. One way of doing this is using a global variable. To change a global variable x from inside a function, the global keyword is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a83140-bdd9-4e11-b53d-6719fe570a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x before: 4\n",
      "x after: 8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to change a global variable x from inside a function, use the global keyword\n",
    "def foo():\n",
    "    global x\n",
    "    x = x*2\n",
    "    \n",
    "x=4\n",
    "print(\"x before:\", x) \n",
    "foo()\n",
    "print(\"x after:\", x)\n",
    "\n",
    "# verify for yourself that omitting the line \"global x\" produces an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776878a2-b6bd-4573-9dc0-cb3a9d68d553",
   "metadata": {},
   "source": [
    "-----\n",
    "# Exercise 1 (exercise 6.6(d), 2.5 pts)\n",
    "**N.B. This is a pen-and-paper exercise. If you prefer you may upload a separate pdf for this exercise and other pen-and-paper exercises. If you do, don't put both files in a single .zip file, upload them both separately.**\n",
    "\n",
    "Consider the minimization of \n",
    "$$\n",
    "  f(x,y) = x^2 + y^2\n",
    "$$\n",
    "subject to\n",
    "$$\n",
    "  g(x,y) = xy^2 -1 = 0 .\n",
    "$$\n",
    "Determine the critical points of the Lagrangian function for this problem and determine whether each is a constrained minimum, a constrained maximum, or neither."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d2058-1b1f-4c27-a5a5-4e1169cf6724",
   "metadata": {},
   "source": [
    "Given $f(x,y)=x^2+y^2$ and $g(x,y)=xy^2-1=0$, our Lagrangian function $\\mathcal{L}$ is as follows:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(x,y,\\lambda)=x^2+y^2+\\lambda(xy^2-1)\n",
    "\\end{equation}\n",
    "$$\n",
    "\\\n",
    "\\\n",
    "Then, the gradient of $\\mathcal{L}$ ($\\nabla \\mathcal{L}$) is a column vector of all partial derivatives.\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\nabla \\mathcal{L} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\delta \\mathcal{L}}{\\delta x} \\\\ \\frac{\\delta \\mathcal{L}}{\\delta y} \\\\ \\frac{\\delta \\mathcal{L}}{\\delta \\lambda}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2 x + \\lambda y^2 \\\\ 2 y + 2 \\lambda x y \\\\ x y^2 - 1\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow\n",
    "\\begin{cases}\n",
    "2 x = - \\lambda y^2 \\\\ 2 y = - 2 \\lambda x y \\\\ x y^2 = 1\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\\\n",
    "\\\n",
    "Solving this system of equations will result in finding the critical points.\n",
    "\\\n",
    "\\\n",
    "First, solving the partial derivative with respect to $y$ ($\\frac{\\delta \\mathcal{L}}{\\delta y}$) gives us,\n",
    "\\\n",
    "$2 y = - 2 \\lambda x y \\Rightarrow 1 = - \\lambda x$\n",
    "\\\n",
    "And hence, $x = - \\frac{1}{\\lambda}$\n",
    "\\\n",
    "\\\n",
    "Filling in this $\\lambda x$ in the partial derivative with respect to $\\lambda$ ($\\frac{\\delta \\mathcal{L}}{\\delta \\lambda}$) provides a value for $y$.\n",
    "\\\n",
    "$x y^2 = 1 \\Rightarrow x y^2 = - \\lambda x \\Rightarrow y^2 = \\frac{1}{x}$\n",
    "\\\n",
    "Then, $y = \\pm \\sqrt{\\frac{1}{x}}$\n",
    "\\\n",
    "\\\n",
    "Lastly, filling in the computed value for $y^2$ in the last unused partial derivative $\\frac{\\delta \\mathcal{L}}{\\delta x}$ results in a value for the last unknown variable $\\lambda$.\n",
    "\\\n",
    "$2 x = - \\lambda \\cdot \\frac{1}{x} \\Rightarrow 2 x = - \\frac{\\lambda}{x} \\Rightarrow 2 x^2 = - \\lambda$\n",
    "\\\n",
    "Now, we can fill in our value vor $x$.\n",
    "\\\n",
    "So that, $2 x^2 = - \\lambda \\Rightarrow 2 \\cdot (- \\frac{1}{\\lambda})^2 = - \\lambda \\Rightarrow \\frac{2}{\\lambda} = - \\lambda \\Rightarrow 2 = \\lambda \\cdot \\lambda^2 \\Rightarrow 2 = - \\lambda^3$\n",
    "\\\n",
    "Hence, $\\lambda = - \\sqrt[3]{2}$\n",
    "\\\n",
    "\\\n",
    "We have found values for all three parameters:\n",
    "\\\n",
    "$\\lambda = - \\sqrt[3]{2}$\n",
    "\\\n",
    "$x = - \\frac{1}{\\lambda} \\Rightarrow x = - \\frac{1}{- \\sqrt[3]{2}} \\Rightarrow x = \\frac{1}{\\sqrt[3]{2}}$\n",
    "\\\n",
    "$y = \\pm \\sqrt{\\frac{1}{x}} \\Rightarrow y = \\pm \\sqrt{\\frac{1}{\\frac{1}{\\sqrt[3]{2}}}} \\Rightarrow y = \\pm \\sqrt{\\sqrt[3]{2}} \\Rightarrow y = \\pm \\sqrt[6]{2}$\n",
    "\\\n",
    "Therefore, the critical points are $(\\frac{1}{\\sqrt[3]{2}},- \\sqrt[6]{2}) \\lor (\\frac{1}{\\sqrt[3]{2}},\\sqrt[6]{2})$\n",
    "\\\n",
    "\\\n",
    "To determine whether the critical points are either a constrained miminim or a constrained maximim or neither, we can check them by the bordered Hessian matrix. Then, we can define the determinant of the bordered Hessian matrix ($det(H)$). If $det(H) > 0$, the critical point is at an extreme value and if $det(H) < 0$, the critical point is not an extreme value. \n",
    "\\\n",
    "Lets define the Hessian matrix $H$ as follows:\n",
    "\\\n",
    "$$\n",
    "\\begin{equation}\n",
    "H = \n",
    "\\begin{bmatrix}\n",
    "2 & 2 \\lambda y & y^2 \\\\ 2 \\lambda y & 2 + 2 \\lambda x & 2 x y \\\\ y^2 & 2 x y & 0\n",
    "\\end{bmatrix} \n",
    "\\end{equation}\n",
    "$$\n",
    "\\\n",
    "Then, we can define the bordered Hessian matrix $B$ accordingly:\n",
    "\\\n",
    "$$\n",
    "\\begin{equation}\n",
    "B = \n",
    "\\begin{bmatrix}\n",
    "2 & 2 \\lambda y \\\\ 2 \\lambda y & 2 + 2 \\lambda x \n",
    "\\end{bmatrix} \n",
    "\\end{equation}\n",
    "$$\n",
    "\\\n",
    "\\\n",
    "Now filling in the values for $x$, $y$ and $\\lambda$ in the bordered Hessian matrix $B$ gives us:\n",
    "\\\n",
    "$$\n",
    "\\begin{equation}\n",
    "B = \n",
    "\\begin{bmatrix}\n",
    "2 & 2 \\cdot - \\sqrt[3]{2} \\cdot \\pm \\sqrt[6]{2} \\\\ 2 - \\sqrt[3]{2} \\cdot \\pm \\sqrt[6]{2} & 2 + 2 \\cdot - \\sqrt[3]{2} \\cdot \\frac{1}{\\sqrt[3]{2}} \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2 & \\pm 2 \\sqrt{2} \\\\ \\pm 2 \\sqrt{2} & 0\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\\\n",
    "\\\n",
    "Computing the determinant of the bordered Hessian matrix ($B$) provides us information of the critical points (minimum or maximum or neither).\n",
    "\\\n",
    "$det(B) = 2 \\cdot 0 - \\pm 2 \\sqrt{2} \\cdot \\pm 2 \\sqrt{2} = 0 - 2^3 = -2^3$\n",
    "\\\n",
    "\\\n",
    "We see that the determinant equals a negative value, meaning that the critical points are neither a constrained minimum nor a constrained maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd779b-f12d-433b-a7a1-48224d0a65b9",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce52e9-1099-4cf9-98b5-e322b7d7673b",
   "metadata": {},
   "source": [
    "## (a) (1 point)\n",
    "The Rosenbrock function is given by\n",
    "\n",
    "$$\n",
    "f(x,y) = 100 (y-x^2)^2 + (1-x)^2\n",
    "$$\n",
    "\n",
    "What is the gradient of $f$? Show that there is exactly one local minimum point and determine this point (N.B. this is a pen-and-paper exercise.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87468d82-a8e3-42d8-9515-d531ca4acf72",
   "metadata": {},
   "source": [
    "The gradient of $f$ is given as $\\nabla F = \\frac{\\delta f}{\\delta x} + \\frac{\\delta f}{\\delta y}$.\n",
    "\\\n",
    "\\\n",
    "Lets first rewrite function $f$.\n",
    "\\\n",
    "$f(x,y) = 100(y-x^2)^2+(1-x)^2 = 100y^2+100x^4-200x^2y+1+x^2-2x = 100x^4+x^2+100y^2-200x^2y-2x+1$\n",
    "\\\n",
    "\\\n",
    "Now, break it down in the two partial derivatives $\\frac{\\delta f}{\\delta x}$ and $\\frac{\\delta f}{\\delta y}$.\n",
    "\\\n",
    "$\\frac{\\delta f}{\\delta x} = 400x^3+2x-400xy-2$\n",
    "\\\n",
    "$\\frac{\\delta f}{\\delta y} = 200y-200x^2$\n",
    "\\\n",
    "Therefore, the gradient of $f$ ($\\nabla f$) can be given by:\n",
    "\\\n",
    "$\\nabla f = (400x^3+2x-400xy-2)+(200y-200x^2) = 400x(x^2-y)+2(x-1)+200(y-x^2)$\n",
    "\\\n",
    "\\\n",
    "To show that there exists exactly one local minimum point, we have to solve $f(x,y)=0$ and find the corresponding $x$ and $y$ value.\n",
    "\\\n",
    "$f(x,y)=0 \\Rightarrow 100(y-x^2)^2+(1-x)^2=0 \\Rightarrow 100(y-x^2)^2=-(1-x)^2 \\Rightarrow (y-x^2)^2=- \\frac{1}{100} (1-x)^2 \\Rightarrow y-x^2=\\pm \\sqrt{- \\frac{1}{100} (1-x)^2} \\Rightarrow y=\\pm \\sqrt{- \\frac{1}{100} (1-x)^2} + x^2$\n",
    "\\\n",
    "\\\n",
    "Substitute this $y$ value back in function $f$ to find a value for $x$ and still $f=0$.\n",
    "\\\n",
    "$ 100(\\pm \\sqrt{- \\frac{1}{100} (1-x)^2} + x^2 - x^2)^2 + (1 - x)^2 = 0 \\Rightarrow 100(\\pm \\sqrt{- \\frac{1}{100} (1-x)^2}) + (1-x)^2 = 0 \\Rightarrow 100(\\pm - \\frac{1}{100} (1-x)^2) + 1 + x^2 - 2x = 0 \\Rightarrow \\pm 1 - x^2 + 1 + x^2 - 2x = 0 \\Rightarrow \\pm 1 + 1 - 2x = 0 \\Rightarrow 2 - 2x = 0 \\lor 0 - 2x = 0 \\Rightarrow -2x = -2 \\lor x = 0 \\Rightarrow x=1 \\lor x=0$\n",
    "\\\n",
    "\\\n",
    "Finally, find the corresponding $y$ values for calculated $x$ values.\n",
    "\\\n",
    "For $x=1$, $y=\\pm \\sqrt{- \\frac{1}{100} (1-x)^2} + x^2 = \\pm \\sqrt{- \\frac{1}{100} (1-1)^2} + 1^2 = 1$.\n",
    "\\\n",
    "For $x=0$, $y=\\pm \\sqrt{- \\frac{1}{100} (1-x)^2} + x^2 = \\pm \\sqrt{- \\frac{0}{100} (1-0)^2} + 0^2 = 0$.\n",
    "\\\n",
    "\\\n",
    "Thus, we found two points in the space that satisfies the requirements for global minimum; $(1,1),(0,0)$. However, one of them is origin $(0,0)$. The other point $(1,1)$ is the local minimum point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b15a0-2bea-4861-a874-a0ac3fdba2f9",
   "metadata": {},
   "source": [
    "## (b) (2 points)\n",
    "Implement the method of steepest descent. Use `scipy.optimize.line_search` as line search method.\n",
    "\n",
    "Test your method on the Rosenbrock function starting from $(x,y) = (0,0)$.\n",
    "Plot the convergence to the minimum: Make a plot of the convergence in the $(x,y)$ plane as well as plot of the norm of the error as a function of the step number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e000842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin minimize f(x,y) = 4x^2 + 3y^2 + 1 demo\n",
      "\n",
      "Gradient descent ala machine learning, LR = 0.10: \n",
      "i =    0  x,y = 2.000000 4.000000 fv = 65.0000\n",
      "i =    1  x,y = 0.400000 1.600000 fv = 9.3200\n",
      "i =    2  x,y = 0.080000 0.640000 fv = 2.2544\n",
      "i =    3  x,y = 0.016000 0.256000 fv = 1.1976\n",
      "i =    4  x,y = 0.003200 0.102400 fv = 1.0315\n",
      "i =    5  x,y = 0.000640 0.040960 fv = 1.0050\n",
      "i =    6  x,y = 0.000128 0.016384 fv = 1.0008\n",
      "i =    7  x,y = 0.000026 0.006554 fv = 1.0001\n",
      "i =    8  x,y = 0.000005 0.002621 fv = 1.0000\n",
      "i =    9  x,y = 0.000001 0.001049 fv = 1.0000\n",
      "i =   10  x,y = 0.000000 0.000419 fv = 1.0000\n",
      "i =   11  x,y = 0.000000 0.000168 fv = 1.0000\n",
      "i =   12  x,y = 0.000000 0.000067 fv = 1.0000\n",
      "i =   13  x,y = 0.000000 0.000027 fv = 1.0000\n",
      "i =   14  x,y = 0.000000 0.000011 fv = 1.0000\n",
      "\n",
      "Using scipy line_search(): \n",
      "i =  199  x,y = 8.400000 8.400000 fv = 494.9200\n",
      "\n",
      "End demo \n"
     ]
    }
   ],
   "source": [
    "#gradient descent demo code https://jamesmccaffrey.wordpress.com/2021/06/16/an-example-of-the-python-scipy-line_search-function/\n",
    "\n",
    "def func(x):\n",
    "  # x is a np.float32 vector\n",
    "    return 4*(x[0]**2) + 3*(x[1]**2) + 1  # scalar\n",
    "\n",
    "def grad(x):\n",
    "    return np.array([8*x[0], 6*x[1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "print(\"\\nBegin minimize f(x,y) = 4x^2 + 3y^2 + 1 demo\")\n",
    "\n",
    "print(\"\\nGradient descent ala machine learning, LR = 0.10: \")\n",
    "xk = np.array([10.0, 10.0], dtype=np.float32)\n",
    "lr = 0.10\n",
    "for i in range(15):\n",
    "    g = grad(xk)\n",
    "    xk = xk - (lr * g)\n",
    "    fv = func(xk)\n",
    "    print(\"i = %4d  x,y = %0.6f %0.6f fv = %0.4f\" % (i, xk[0], \\\n",
    "    xk[1], fv))\n",
    "\n",
    "print(\"\\nUsing scipy line_search(): \")\n",
    "xk = np.array([10.0, 10.0], dtype=np.float32)  # start\n",
    "pk = np.array([-0.10, -0.10], dtype=np.float32)  # direction\n",
    "\n",
    "for i in range(200):\n",
    "    results = line_search(func, grad, xk, pk)  # will warn\n",
    "    # print(results)\n",
    "    alpha = results[0]\n",
    "    if alpha is None:\n",
    "        print(\"line_search done \")\n",
    "        break\n",
    "\n",
    "xk = xk + alpha * pk\n",
    "fv = func(xk)\n",
    "print(\"i = %4d  x,y = %0.6f %0.6f fv = %0.4f\" % (i, xk[0], xk[1], fv))\n",
    "\n",
    "print(\"\\nEnd demo \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ccad847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results line search (alpha, n_function_evals, n_gradient_evals, new_fval, old_fval, new_slope)\n",
      "\n",
      "pk: [ 2. -0.]\n",
      "xk: [0. 0.]\n",
      "(0.00228828029537145, 10, 1, 0.9900408341956339, 1.0, array([-2.0288804,  2.262099 ], dtype=float32))\n",
      "alpha = 0.00228828029537145\n",
      "new_fval: 0.9900408341956339; old_fval: 1.0\n",
      "\n",
      "pk: [1.9908085  0.00418898]\n",
      "xk: [0.00457656 0.        ]\n",
      "(0.08447076567721791, 6, 2, 0.7737246913528644, 0.990867867624583, array([ 0.4367859, -6.0217056], dtype=float32))\n",
      "alpha = 0.08447076567721791\n",
      "new_fval: 0.7737246913528644; old_fval: 0.990867867624583\n",
      "\n",
      "pk: [-0.38285694  5.8971686 ]\n",
      "xk: [0.17274168 0.00035385]\n",
      "(None, 13, 0, None, 0.7712978136714453, None)\n",
      "alpha = None\n",
      "new_fval: None; old_fval: 0.7712978136714453\n",
      "\n",
      "CONVERGED\n",
      "\n",
      "i =    2  x,y = 0.172742 0.000354 fv = 0.7713\n"
     ]
    }
   ],
   "source": [
    "# copied and altered code according to exercise (https://jamesmccaffrey.wordpress.com/2021/06/16/an-example-of-the-python-scipy-line_search-function/)\n",
    "\n",
    "def func(x):\n",
    "    return 100*(x[1]-x[0]**2)**2+(1-x[0])**2\n",
    "\n",
    "def grad(x):\n",
    "    return np.array([400*x[0]*((x[0]**2)-x[1])+2*(x[0]-1),200*(x[1]-x[0]**2)], dtype=np.float32)\n",
    "\n",
    "xk = np.array([0.0, 0.0], dtype=np.float32)        # initial guess\n",
    "pk = np.array([5.0, 5.0], dtype=np.float32)       # initial direction\n",
    "\n",
    "print(\"Results line search (alpha, n_function_evals, n_gradient_evals, new_fval, old_fval, new_slope)\\n\")\n",
    "for i in range(200):\n",
    "    results = scipy.optimize.line_search(func, grad, xk, pk)\n",
    "    pk = - grad(xk)\n",
    "    print(f\"pk: {pk}\")\n",
    "    print(f\"xk: {xk}\")\n",
    "    print(f\"{results}\")\n",
    "        # alpha - float or None\n",
    "            # Alpha for which x_new = x0 + alpha * pk, or None if the line search algorithm did not converge.\n",
    "        # fc - int\n",
    "            # Number of function evaluations made.\n",
    "        # gc - int\n",
    "            # Number of gradient evaluations made.\n",
    "        # new_fval - float or None\n",
    "            # New function value f(x_new)=f(x0+alpha*pk), or None if the line search algorithm did not converge.\n",
    "        # old_fval - float\n",
    "            # Old function value f(x0).\n",
    "        # new_slope - float or None\n",
    "            # The local slope along the search direction at the new value <myfprime(x_new), pk>, or None if the line search algorithm did not converge.\n",
    "    alpha = results[0]\n",
    "    print(f\"alpha = {alpha}\")\n",
    "    print(f\"new_fval: {results[3]}; old_fval: {results[4]}\\n\")\n",
    "    if alpha is None:\n",
    "        print(\"CONVERGED\\n\")\n",
    "        break\n",
    "\n",
    "    xk = xk + alpha * pk\n",
    "    fv = func(xk)\n",
    "print(\"i = %4d  x,y = %0.6f %0.6f fv = %0.4f\" % (i, xk[0], xk[1], fv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2ef08-a74e-4712-ad5f-acae905b5142",
   "metadata": {},
   "source": [
    "## (c) (1.5 points)\n",
    "\n",
    "Implement the BFGS method for unconstrained optimization, given in Heath chapter 6. Test the correctness of the code using the data in Example 6.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30895688-1c83-41b3-873d-17f37a906796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check deze github pagina https://github.com/trsav/bfgs/blob/master/BFGS.py\n",
    "\n",
    "# x0 = initial guess\n",
    "# B0 = initial Hessian approximation\n",
    "# for k = 0,1,2,...\n",
    "    # Solve Bk sk = −∇f(xk) for sk                               { compute quasi-Newton step }\n",
    "    # xk+1 = xk + sk                                             { update solution }\n",
    "    # yk = ∇f(xk+1) − ∇f(xk) \n",
    "    # Bk+1 = Bk + (ykykT )/(ykT sk) − (BksksTk Bk)/(sTk Bksk)    { update approximate Hessian }\n",
    "# end\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def func(x_1,x_2):\n",
    "    return 0.5*x_1^2 + 2.5*x_2^2\n",
    "\n",
    "def grad(x_1,x_2):\n",
    "    return [x_1,5*x_2]\n",
    "\n",
    "x0 = np.transpose([5 1])\n",
    "x = [x0]\n",
    "B0 = np.identity(2)\n",
    "B = [B0]\n",
    "y = []\n",
    "max_iter = 100\n",
    "\n",
    "for k in max_iter:\n",
    "    Solve B[k] * s[k] = − grad(x[k]) for s[k]\n",
    "    x[k+1] = x[k] + s[k] \n",
    "    y[k] = grad(x[k+1]) − grad(x[k]) \n",
    "    B[k+1] = B[k] + (y[k]*np.transpose(y[k]))/(np.transpose(y[k]) * s[k]) − (B[k]*s[k]*np.transpose(s)[k] * B[k])/(np.transpose(s)[k] * B[k] * s[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efef4b-0200-4d10-97a1-f2c9a78d8579",
   "metadata": {},
   "source": [
    "## (d) (1 points)\n",
    "\n",
    "Apply your implementation of the BFGS method to find a local minimum of the Rosenbrock function (see previous exercise). Use starting point $(0,0)$ and do not assume any knowledge of the Hessian when you choose $B_0$.\n",
    "Plot the convergence to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb1e4b-cfe1-4aad-8b50-d7c01be3b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dfc5ce-542f-454c-924e-25de56a6fe7c",
   "metadata": {},
   "source": [
    "## (e) (1 point)\n",
    "How does the convergence compare to that of gradient descent (see\n",
    "previous question)? Let your program count the number of function and gradient evaluations and\n",
    "consider this in your comparison. Implement a stopping criterion in both methods that runs until $||x_k-x^*||_2 < 10^{-5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3b21f-2306-40e8-a659-05de85f9ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3e97b-9589-4bed-bf09-d7297719bee1",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
